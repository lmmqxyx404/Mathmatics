# traditional probability

# probability and statistics

# random test
1. before the test, we can predict all possible results.
2. before the test, we can not know the precise result.
3. The test could be repeated in the same condition.

## 1.1 sample spaces(always noted as S)
###  random event.
basic event
certain event
impossible event
The subset(not the element in S) of S is A.

### the relation between events and relative computing.
Union event
Difference event
Product event
Inverse event

## the axiomatic definition of probability
The axiomatic definition of probability, often referred to as the Kolmogorov's axioms, provides a formal framework for describing probability. According to this definition, probability is defined based on three axioms:

1. Axiom of Non-Negativity: For any event A, the probability of A is a non-negative real number. In mathematical terms, this means that P(A) ≥ 0.

2. Axiom of Additivity: If A and B are mutually exclusive events (i.e., they cannot occur simultaneously), then the probability of the union of A and B is equal to the sum of their individual probabilities. Mathematically, this can be expressed as P(A ∪ B) = P(A) + P(B).

3. Axiom of Normalization: The probability of the entire sample space (the set of all possible outcomes) is equal to 1. In other words, the probability of the sample space S is P(S) = 1.

These axioms provide the foundation for defining and manipulating probabilities. From these axioms, various properties and rules of probability can be derived, such as the complement rule, the multiplication rule, and the inclusion-exclusion principle.

It's worth noting that these axioms are just one way to define probability, and alternative approaches exist, such as the subjective interpretation of probability. However, the axiomatic definition based on Kolmogorov's axioms is widely used in probability theory and forms the basis of many statistical and probabilistic analyses.


### TODO: consider the following formula
$$
P( \bigcup_{\text{i=1}}^{n} A_i)=\sum_{i=1}^{n}P(A_i)+...
$$

### basic point
## Probability distribution
 - 1. Binomial distribution 
 - 2. Bernoulli distribution
 - 3. Unify
 - 4. Exponential
 - 5. Normal distribution

# Probability density function
The relation of variable and probability density function
in general,
  $$F(x) = \int_{-{\infty}}^{x}f(x){\mathrm{d}x} $$

# Expectation
## Random variables with finitely many outcomes


## Random variables with density
  $$E[X] = \int_{-{\infty}}^{+{\infty}}x*f(x){\mathrm{d}x} $$

Ram
Rsm

# point estimate method

# Law of the unconscious statistician
In probability theory and statistics, the law of the unconscious statistician, or LOTUS, is a theorem which expresses the expected value of a function g(X) of a random variable X in terms of g and the probability distribution of X.

# d
